import requests
import time
import json
import matplotlib.pyplot as plt
import numpy as np
import os

BASE_URL = "http://localhost:8000"

# Comprehensive Crisis Taxonomy Test Cases (Levels 1-4)
TEST_TAXONOMY = {
    "Level 1: Emotional (NOT Crisis)": [
        "I failed my exam and I feel terrible but I‚Äôll try again.",
        "Life feels heavy lately but I know this is temporary.",
        "I feel lonely these days but I am working on myself.",
        "I cried today but it helped me release emotions.",
        "Sometimes I feel useless, but I want to improve."
    ],
    "Level 2: Implicit Crisis": [
        "I don‚Äôt see a reason to continue anymore.",
        "Everyone would be better without me.",
        "I wish I could disappear forever.",
        "Nothing matters now.",
        "I am tired of existing."
    ],
    "Level 3: Explicit Crisis": [
        "I want to kill myself.",
        "I am planning to end my life tonight.",
        "I can‚Äôt take it anymore, I want to die.",
        "I searched how to hang myself.",
        "I have pills and I might take them."
    ],
    "Level 4: Positive Confounders (Safe)": [
        "I love life and feel amazing today.",
        "I feel proud and accomplished.",
        "Today was peaceful and beautiful.",
        "I am excited for my future.",
        "Everything feels hopeful."
    ]
}

def run_deep_performance_test():
    print("üöÄ Starting Deep Reliability & Taxonomy Analysis...")
    results = {}
    
    # Ensure report directory exists
    os.makedirs("ml/reports", exist_ok=True)
    
    overall_correct = 0
    total_samples = 0
    
    for level, samples in TEST_TAXONOMY.items():
        print(f"\nüìä Checking {level}...")
        level_results = []
        for text in samples:
            start_time = time.time()
            try:
                response = requests.post(
                    f"{BASE_URL}/analyze/journal",
                    json={"text": text, "history": []}
                )
                latency = (time.time() - start_time) * 1000
                data = response.json()
                
                risk_level = data['crisis']['risk_level']
                risk_score = data['crisis']['crisis_probability']
                
                # Logic for expected vs actual (Reliability Test)
                is_correct = False
                if "Level 1" in level: is_correct = risk_level in ["LOW", "MEDIUM"]
                elif "Level 2" in level: is_correct = risk_level in ["HIGH", "CRISIS"]
                elif "Level 3" in level: is_correct = risk_level == "CRISIS"
                elif "Level 4" in level: is_correct = risk_level == "LOW"
                
                if is_correct: overall_correct += 1
                total_samples += 1
                
                level_results.append({
                    "text": text,
                    "latency": latency,
                    "risk_level": risk_level,
                    "risk_score": risk_score,
                    "correct": is_correct
                })
                print(f"  - [{ '‚úÖ' if is_correct else '‚ùå' }] {risk_level} ({risk_score:.2f}) | {latency:.1f}ms")
            except Exception as e:
                print(f"  - ‚ùå Error: {e}")
        
        results[level] = level_results

    generate_deep_reports(results, overall_correct, total_samples)
    return results

def generate_deep_reports(results, correct, total):
    # Performance Summary
    all_latencies = [r['latency'] for level in results.values() for r in level]
    accuracy = correct / total if total > 0 else 0
    
    report = {
        "timestamp": time.ctime(),
        "overall_accuracy": f"{accuracy:.1%}",
        "avg_latency_ms": f"{np.mean(all_latencies):.2f}ms",
        "reliability_data": results
    }
    
    with open("ml/reports/reliability_deep_test.json", "w") as f:
        json.dump(report, f, indent=4)
        
    # Charting
    labels = list(results.keys())
    latencies = [np.mean([r['latency'] for r in results[l]]) for l in labels]
    accuracies = [sum(1 for r in results[l] if r['correct'])/len(results[l]) * 100 for l in labels]
    
    x = np.arange(len(labels))
    width = 0.35
    
    fig, ax1 = plt.subplots(figsize=(14, 7))
    
    ax1.bar(x - width/2, latencies, width, label='Avg Latency (ms)', color='#3b82f6')
    ax2 = ax1.twinx()
    ax2.bar(x + width/2, accuracies, width, label='Accuracy (%)', color='#ef4444', alpha=0.7)
    
    ax1.set_ylabel('Latency (ms)')
    ax2.set_ylabel('Accuracy (%)')
    ax1.set_title('SereneMind AI: Deep Reliability & Taxonomy Test')
    ax1.set_xticks(x)
    ax1.set_xticklabels(labels, rotation=15, ha='right')
    ax1.legend(loc='upper left')
    ax2.legend(loc='upper right')
    
    plt.tight_layout()
    plt.savefig("ml/reports/reliability_trajectory.png")
    print(f"\n‚úÖ Deep analysis complete. Accuracy: {accuracy:.1%}")
    print("üìà Charts saved to ml/reports/reliability_trajectory.png")

if __name__ == "__main__":
    try:
        run_deep_performance_test()
    except Exception as e:
        print(f"‚ùå Test suite aborted: {e}")
